{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralpredictors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11348\\1354022990.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mneuralpredictors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMovieFileTreeDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mneuralpredictors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSubsetSequentialSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m from neuralpredictors.data.transforms import (AddBehaviorAsChannels,\n\u001b[0;32m      5\u001b[0m                                               \u001b[0mAddPupilCenterAsChannels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neuralpredictors'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from neuralpredictors.data.datasets import MovieFileTreeDataset\n",
    "from neuralpredictors.data.samplers import SubsetSequentialSampler\n",
    "from neuralpredictors.data.transforms import (AddBehaviorAsChannels,\n",
    "                                              AddPupilCenterAsChannels,\n",
    "                                              ChangeChannelsOrder, CutVideos,\n",
    "                                              ExpandChannels, NeuroNormalizer,\n",
    "                                              ScaleInputs, SelectInputChannel,\n",
    "                                              Subsample, Subsequence, ToTensor)\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mouse_video_loader(\n",
    "    paths,\n",
    "    batch_size,\n",
    "    normalize=True,\n",
    "    exclude: str = None,\n",
    "    cuda: bool = False,\n",
    "    max_frame=None,\n",
    "    frames=50,\n",
    "    offset=-1,\n",
    "    inputs_mean=None,\n",
    "    inputs_std=None,\n",
    "    include_behavior=True,\n",
    "    include_pupil_centers=True,\n",
    "    include_pupil_centers_as_channels=False,\n",
    "    scale=1,\n",
    "    to_cut=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Symplified version of the sensorium mouse_loaders.py\n",
    "     Returns a dictionary of dataloaders (i.e., trainloaders, valloaders, and testloaders) for >= 1 dataset(s).\n",
    "    Args:\n",
    "        paths (list): list of paths for the datasets\n",
    "        batch_size (int): batch size.\n",
    "        frames (int, optional): how many frames ot take per video\n",
    "        max_frame (int, optional): which is the maximal frame that could be taken per video\n",
    "        offset (int, optional): Offset to start the subsequence from. Defaults to -1, corresponding to random but valid offset at each iteration.\n",
    "        cuda (bool, optional): whether to place the data on gpu or not.\n",
    "        normalize (bool, optional): whether to normalize the data (see also exclude)\n",
    "        exclude (str, optional): data to exclude from data-normalization. Only relevant if normalize=True. Defaults to 'images'\n",
    "        include_behavior (bool, optional): whether to include behavioral data\n",
    "        include_pupil_centers (bool, optional): whether to include pupil center data\n",
    "        include_pupil_centers_as_channels(bool, optional): whether to include pupil center data as channels\n",
    "        scale(float, optional): scalar factor for the image resolution.\n",
    "            scale = 1: full iamge resolution (144 x 256)\n",
    "            scale = 0.25: resolution used for model training (36 x 64)\n",
    "        float64 (bool, optional):  whether to use float64 in MovieFileTreeDataset\n",
    "    Returns:\n",
    "        dict: dictionary of dictionaries where the first level keys are 'train', 'validation', and 'test', and second level keys are data_keys.\n",
    "    \"\"\"\n",
    "    assert frames > 50, 'frames must be higher than 50'\n",
    "    data_keys = [\n",
    "        \"videos\",\n",
    "        \"responses\",\n",
    "    ]\n",
    "    if include_behavior:\n",
    "        data_keys.append(\"behavior\")\n",
    "    if include_pupil_centers:\n",
    "        data_keys.append(\"pupil_center\")\n",
    "\n",
    "    #     dataloaders_combined = {\"validation\": {}, \"train\": {}, \"test\": {}}\n",
    "    dataloaders_combined = {}\n",
    "\n",
    "    for path in paths:\n",
    "        dat2 = MovieFileTreeDataset(path, *data_keys)\n",
    "\n",
    "        conds = np.ones(len(dat2.neurons.cell_motor_coordinates), dtype=bool)\n",
    "        idx = np.where(conds)[0]\n",
    "\n",
    "        more_transforms = [\n",
    "            Subsample(idx, target_index=0),\n",
    "            CutVideos(\n",
    "                max_frame=max_frame,\n",
    "                frame_axis={data_key: -1 for data_key in data_keys},\n",
    "                target_groups=data_keys,\n",
    "            ),\n",
    "            ChangeChannelsOrder((2, 0, 1), in_name=\"videos\"),\n",
    "            ChangeChannelsOrder((1, 0), in_name=\"responses\"),\n",
    "            ChangeChannelsOrder((1, 0), in_name=\"behavior\"),\n",
    "            ChangeChannelsOrder((1, 0), in_name=\"pupil_center\"),\n",
    "        ]\n",
    "        if to_cut:\n",
    "            more_transforms.append(\n",
    "                Subsequence(frames=frames, channel_first=(), offset=offset)\n",
    "            )\n",
    "        more_transforms = more_transforms + [\n",
    "            ChangeChannelsOrder((1, 0), in_name=\"responses\"),\n",
    "            ChangeChannelsOrder((1, 0), in_name=\"behavior\"),\n",
    "            ChangeChannelsOrder((1, 0), in_name=\"pupil_center\"),\n",
    "            ExpandChannels(\"videos\"),\n",
    "        ]\n",
    "\n",
    "        if include_behavior:\n",
    "            more_transforms.append(\n",
    "                AddBehaviorAsChannels(\n",
    "                    \"videos\"\n",
    "                )\n",
    "            )\n",
    "        if include_pupil_centers and include_pupil_centers_as_channels:\n",
    "            more_transforms.append(AddPupilCenterAsChannels(\"videos\"))\n",
    "\n",
    "        more_transforms.append(ToTensor(cuda))\n",
    "        more_transforms.insert(\n",
    "            0, ScaleInputs(scale=scale, in_name=\"videos\", channel_axis=-1)\n",
    "        )\n",
    "        if normalize:\n",
    "            try:\n",
    "                more_transforms.insert(\n",
    "                    0,\n",
    "                    NeuroNormalizer(\n",
    "                        dat2,\n",
    "                        exclude=exclude,\n",
    "                        inputs_mean=inputs_mean,\n",
    "                        inputs_std=inputs_std,\n",
    "                        in_name=\"videos\",\n",
    "                    ),\n",
    "                )\n",
    "            except:\n",
    "                more_transforms.insert(\n",
    "                    0, NeuroNormalizer(dat2, exclude=exclude, in_name=\"videos\")\n",
    "                )\n",
    "\n",
    "        dat2.transforms.extend(more_transforms)\n",
    "\n",
    "        # subsample images\n",
    "        tier = None\n",
    "        dataloaders = {}\n",
    "        keys = [tier] if tier else list(set(list(dat2.trial_info.tiers)))\n",
    "        tier_array = dat2.trial_info.tiers\n",
    "\n",
    "        for tier in keys:\n",
    "            if tier != 'none':\n",
    "                subset_idx = np.where(tier_array == tier)[0]\n",
    "\n",
    "                sampler = (\n",
    "                    SubsetRandomSampler(subset_idx)\n",
    "                    if tier == \"train\"\n",
    "                    else SubsetSequentialSampler(subset_idx)\n",
    "                )\n",
    "                dataloaders[tier] = DataLoader(\n",
    "                    dat2,\n",
    "                    sampler=sampler,\n",
    "                    batch_size=batch_size,\n",
    "                )\n",
    "\n",
    "        dataset_name = path.split(\"/\")[-2]\n",
    "        for k, v in dataloaders.items():\n",
    "            if k not in dataloaders_combined.keys():\n",
    "                dataloaders_combined[k] = {}\n",
    "            dataloaders_combined[k][dataset_name] = v\n",
    "\n",
    "    return dataloaders_combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
